{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9JhLXoJI81f",
        "outputId": "9b3c47ec-afee-4e59-e01f-65f6e30cd54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "# Execute this cell only if you want to synchronise Google Drive. Otherwise, enter the path to your file below.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print('Note: using Google CoLab')\n",
        "    path_files = '/content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/'\n",
        "except:\n",
        "    print('Note: not using Google CoLab')\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r '/content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4E6A6Tm13CZ",
        "outputId": "6b4b6db0-3fb4-4dcd-d92d-44502c2ea8d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: pytorch-lightning==1.8.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: transformers==4.22.2 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (4.22.2)\n",
            "Requirement already satisfied: datasets==2.9.0 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (2.9.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.97 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 5)) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn==1.2.0 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 7)) (1.23.5)\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 9)) (3.8.1)\n",
            "Requirement already satisfied: stanza==1.4.2 in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 10)) (1.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (11.10.3.66)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (2.12.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: lightning-utilities==0.3.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (0.11.4)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (2023.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (9.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 6)) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 8)) (2022.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 9)) (8.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 10)) (1.16.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 10)) (3.20.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 10)) (2.2.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.9/dist-packages (from lightning-utilities==0.3.*->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 1)) (67.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (1.9.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 4)) (6.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 3)) (1.26.15)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (1.53.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (6.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning==1.8.1->-r /content/drive/Othercomputers/Mon ordinateur portable/CS/SM11/NLP/Ponchon_Deneve_Setrouk_Demy/requirements.txt (line 2)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def clean_sentences(df):\n",
        "    clean_df = df.copy()\n",
        "\n",
        "    for column in ['target', 'sentence']:\n",
        "        # Sentence and target to lower to avoid capital letters issue.\n",
        "        clean_df[column] = clean_df[column].apply(lambda x: x.lower())\n",
        "        # Remove punctuation using regex.\n",
        "        clean_df[column] = clean_df[column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "        # Remove numbers using regex.\n",
        "        clean_df[column] = clean_df[column].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "        # Lemmatize the verbs.\n",
        "        clean_df[column] = clean_df[column].apply(lambda x: \" \".join([WordNetLemmatizer().lemmatize(word, 'v') for word in x.split()]))\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "\n",
        "class Classifier():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mapping_dict = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "        self.reverse_mapping_dict = {v: k for k, v in self.mapping_dict .items()}\n",
        "        self.tokenizer_self_bert = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3,\n",
        "            output_attentions=False, output_hidden_states=False)\n",
        "        self.batch_size = 16\n",
        "        self.epochs = 8\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr = 5e-5, eps = 1e-08) # Very low learning rate to finetune the model don't disturb too much the pretrained weights.\n",
        "\n",
        "\n",
        "    def tokenize(self, df):\n",
        "        token_df = df.copy()\n",
        "        token_df['bert_encoded_dict'] = token_df['bert_encoded'].apply(\n",
        "            lambda x: self.tokenizer_self_bert.encode_plus(text=x, add_special_tokens=True,\n",
        "            padding='max_length', max_length=self.max_sentence_length, return_attention_mask=True))\n",
        "        token_df = pd.concat([token_df.drop(['bert_encoded_dict'], axis=1), token_df['bert_encoded_dict'].apply(pd.Series)], axis=1)\n",
        "        del token_df['token_type_ids']\n",
        "        print(f\"Input vectors final length: {np.vstack(token_df['input_ids'].apply(np.ravel)[0]).shape}\")\n",
        "        return token_df\n",
        "\n",
        "\n",
        "    def train(self, train_filename: str, dev_filename: str, device: torch.device = device):\n",
        "        \"\"\"\n",
        "        Trains the classifier model on the training set stored in file train_filename.\n",
        "        \"\"\"\n",
        "\n",
        "        # We load the data and clean the text\n",
        "        data = pd.read_csv(train_filename, sep='\\t', header=None, names=['polarity', 'aspect', 'target', 'position', 'sentence'])\n",
        "        clean_data = clean_sentences(data)\n",
        "\n",
        "        # Before encoding, we need to know the size of the longest sequence to pad accordingly\n",
        "        clean_data['bert_encoded'] = clean_data['sentence'].astype(str)  + '[SEP]' + clean_data['aspect'].astype(str) + '[SEP]' + clean_data['target'].astype(str)\n",
        "        clean_data['bert_encoded_split'] = clean_data['bert_encoded'].str.split(' ')\n",
        "        self.max_sentence_length = 128 # max([len(i) for i in clean_data['bert_encoded_split'].values]) # To find why not working\n",
        "        print(f'Maximum sentence length in training data: {self.max_sentence_length}')\n",
        "        # print(f'\\n{clean_data.head()}\\n')\n",
        "\n",
        "        # Now we need to tokenize the text using BertTokenizer and to format the input vectors\n",
        "        tokenize_data = self.tokenize(clean_data)\n",
        "        # print(tokenize_data.head())\n",
        "        tokenize_data['polarity'] = tokenize_data['polarity'].map(self.mapping_dict)\n",
        "        token_ids = torch.tensor(np.vstack(tokenize_data['input_ids'].apply(np.ravel))).to(device)\n",
        "        token_attention = torch.tensor(np.vstack(tokenize_data['attention_mask'].apply(np.ravel))).to(device)\n",
        "        token_labels = torch.tensor(tokenize_data['polarity'].values).to(device)\n",
        "\n",
        "        # Train set and prepare DataLoader\n",
        "        train_set = TensorDataset(token_ids, token_attention, token_labels)\n",
        "        train_dataloader = DataLoader(train_set, sampler=RandomSampler(train_set), batch_size=self.batch_size)\n",
        "\n",
        "        # ---------- TRAINING LOOP ----------\n",
        "        self.model = self.model.to(device)\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            tr_loss = 0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "                self.optimizer.zero_grad()\n",
        "                # Forward pass\n",
        "                train_output = self.model(b_input_ids, token_type_ids=None,\n",
        "                                          attention_mask=b_input_mask, labels=b_labels)\n",
        "                # Backward pass\n",
        "                train_output.loss.backward()\n",
        "                self.optimizer.step()\n",
        "                # Update tracking variables\n",
        "                tr_loss += train_output.loss.item()\n",
        "            print(f'Epoch {epoch}: training loss = {tr_loss}')\n",
        "\n",
        "\n",
        "    def predict(self, data_filename: str, device: torch.device=device) -> List[str]:\n",
        "        \"\"\"\n",
        "        Predicts class labels for the input instances in file 'data_filename'.\n",
        "        Returns the list of predicted labels.\n",
        "        \"\"\"\n",
        "        \n",
        "        # We load the test data and clean the text\n",
        "        data_test = pd.read_csv(data_filename, sep = \"\\t\", names = ['polarity', 'aspect', 'target', 'position', 'sentence'])\n",
        "        clean_test_data = clean_sentences(data_test)\n",
        "\n",
        "        # Again we use BertTokenizer to tokenize the text: target words and sentences\n",
        "        clean_test_data['bert_encoded'] = clean_test_data['sentence'].astype(str)  + '[SEP]' + clean_test_data['aspect'].astype(str) + '[SEP]' + clean_test_data['target'].astype(str)\n",
        "        # print(f'\\n{clean_test_data.head()}\\n')\n",
        "        tokenize_test_data = self.tokenize(clean_test_data)\n",
        "        # print(tokenize_test_data.head())\n",
        "        \n",
        "        # Format the test input vectors and prepare DataLoader\n",
        "        test_token_ids = torch.tensor(np.vstack(tokenize_test_data['input_ids'].apply(np.ravel))).to(device)\n",
        "        test_token_attention = torch.tensor(np.vstack(tokenize_test_data['attention_mask'].apply(np.ravel))).to(device)\n",
        "        test_set = TensorDataset(test_token_ids, test_token_attention)\n",
        "        test_dataloader = DataLoader(test_set, sampler=SequentialSampler(test_set), batch_size=self.batch_size)\n",
        "\n",
        "        # ---------- INFERENCE LOOP ----------\n",
        "        self.model.eval()\n",
        "        self.pred = []\n",
        "        for batch in test_dataloader:\n",
        "            b_input_ids, b_input_mask = batch\n",
        "            with torch.no_grad():\n",
        "                # Forward pass\n",
        "                eval_output = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            pred_polarity = np.argmax(eval_output.logits.cpu().detach().numpy(), axis=1)\n",
        "            self.pred += [self.reverse_mapping_dict[x] for x in pred_polarity]\n",
        "\n",
        "        return self.pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923dad8b-1edf-4974-fc98-b71a500c2855",
        "id": "rFmtgwbRwtBp"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, sys\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_reproducible():\n",
        "    # The below is necessary to have reproducible behavior.\n",
        "    import random as rn\n",
        "    import os\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    # The below is necessary for starting Numpy generated random numbers\n",
        "    # in a well-defined initial state.\n",
        "    np.random.seed(17)\n",
        "    # The below is necessary for starting core Python generated random numbers\n",
        "    # in a well-defined state.\n",
        "    rn.seed(12345)\n",
        "\n",
        "\n",
        "def load_label_output(filename):\n",
        "    with open(filename, 'r', encoding='UTF-8') as f:\n",
        "        return [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n",
        "\n",
        "\n",
        "def eval_list(glabels, slabels):\n",
        "    if (len(glabels) != len(slabels)):\n",
        "        print(\"\\nWARNING: label count in system output (%d) is different from gold label count (%d)\\n\" % (\n",
        "        len(slabels), len(glabels)))\n",
        "    n = min(len(slabels), len(glabels))\n",
        "    incorrect_count = 0\n",
        "    for i in range(n):\n",
        "        if slabels[i] != glabels[i]: incorrect_count += 1\n",
        "    acc = (n - incorrect_count) / n\n",
        "    return acc*100\n",
        "\n",
        "\n",
        "def train_and_eval(classifier, trainfile, devfile, testfile, run_id, device):\n",
        "    print(f\"\\nRUN: {run_id}\")\n",
        "    print(\"  %s.1. Training the classifier...\" % str(run_id))\n",
        "    classifier.train(trainfile, devfile, device)\n",
        "    print()\n",
        "    print(\"  %s.2. Eval on the dev set...\" % str(run_id), end=\"\")\n",
        "    slabels = classifier.predict(devfile, device)\n",
        "    glabels = load_label_output(devfile)\n",
        "    devacc = eval_list(glabels, slabels)\n",
        "    print(\" Acc.: %.2f\" % devacc)\n",
        "    testacc = -1\n",
        "    if testfile is not None:\n",
        "        # Evaluation on the test data\n",
        "        print(\"  %s.3. Eval on the test set...\" % str(run_id), end=\"\")\n",
        "        slabels = classifier.predict(testfile)\n",
        "        glabels = load_label_output(testfile)\n",
        "        testacc = eval_list(glabels, slabels)\n",
        "        print(\" Acc.: %.2f\" % testacc)\n",
        "    print()\n",
        "    return (devacc, testacc)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_runs = 5\n",
        "    set_reproducible()\n",
        "    datadir = path_files\n",
        "    trainfile =  datadir + \"data/traindata.csv\"\n",
        "    devfile =  datadir + \"data/devdata.csv\"\n",
        "    testfile = None\n",
        "    # testfile = datadir + \"testdata.csv\"\n",
        "\n",
        "    # Runs\n",
        "    start_time = time.perf_counter()\n",
        "    devaccs = []\n",
        "    testaccs = []\n",
        "    for i in range(1, n_runs+1):\n",
        "        classifier =  Classifier()\n",
        "        devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i, device)\n",
        "        devaccs.append(np.round(devacc,2))\n",
        "        testaccs.append(np.round(testacc,2))\n",
        "    print('\\nCompleted %d runs.' % n_runs)\n",
        "    total_exec_time = (time.perf_counter() - start_time)\n",
        "    print(\"Dev accs:\", devaccs)\n",
        "    print(\"Test accs:\", testaccs)\n",
        "    print()\n",
        "    print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n",
        "    print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n",
        "    print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nryMse-2TgQ",
        "outputId": "78da8652-f71b-4238-df88-5c7820b02ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RUN: 1\n",
            "  1.1. Training the classifier...\n",
            "Maximum sentence length in training data: 128\n",
            "Input vectors final length: (128, 1)\n",
            "Epoch 0: training loss = 62.855787977576256\n",
            "Epoch 1: training loss = 40.27422474324703\n",
            "Epoch 2: training loss = 26.343893358949572\n",
            "Epoch 3: training loss = 18.21130006434396\n",
            "Epoch 4: training loss = 11.52524868492037\n",
            "Epoch 5: training loss = 13.57446880324278\n",
            "Epoch 6: training loss = 7.185713369981386\n",
            "Epoch 7: training loss = 3.102986018988304\n",
            "\n",
            "  1.2. Eval on the dev set...Input vectors final length: (128, 1)\n",
            " Acc.: 82.98\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RUN: 2\n",
            "  2.1. Training the classifier...\n",
            "Maximum sentence length in training data: 128\n",
            "Input vectors final length: (128, 1)\n",
            "Epoch 0: training loss = 76.52230414748192\n",
            "Epoch 1: training loss = 49.71830156445503\n",
            "Epoch 2: training loss = 33.86152597516775\n",
            "Epoch 3: training loss = 23.330589368008077\n",
            "Epoch 4: training loss = 15.818208580836654\n",
            "Epoch 5: training loss = 13.094537431956269\n",
            "Epoch 6: training loss = 10.408812752459198\n",
            "Epoch 7: training loss = 7.915469855535775\n",
            "\n",
            "  2.2. Eval on the dev set...Input vectors final length: (128, 1)\n",
            " Acc.: 81.12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RUN: 3\n",
            "  3.1. Training the classifier...\n",
            "Maximum sentence length in training data: 128\n",
            "Input vectors final length: (128, 1)\n",
            "Epoch 0: training loss = 67.62197180464864\n",
            "Epoch 1: training loss = 42.42142842710018\n",
            "Epoch 2: training loss = 28.888348788022995\n",
            "Epoch 3: training loss = 18.7381225368008\n",
            "Epoch 4: training loss = 14.833952088607475\n",
            "Epoch 5: training loss = 9.807166145415977\n",
            "Epoch 6: training loss = 9.478793879738078\n",
            "Epoch 7: training loss = 6.975359396717977\n",
            "\n",
            "  3.2. Eval on the dev set...Input vectors final length: (128, 1)\n",
            " Acc.: 83.24\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RUN: 4\n",
            "  4.1. Training the classifier...\n",
            "Maximum sentence length in training data: 128\n",
            "Input vectors final length: (128, 1)\n",
            "Epoch 0: training loss = 65.49192059785128\n"
          ]
        }
      ]
    }
  ]
}